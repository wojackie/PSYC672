\section{Method}
\subsection{Participants}
Eighty-eight Chinese participants (Mean age ±SD: 21.2 ±1.2, 54 females: 61\%) were recruited based on the following criteria: (i) aged 18 or above, (ii) undergraduate students at Hong Kong universities, (iii) with normal or correct-to-normal vision, (iv) capable of reading traditional Chinese, and (v) with no self-reported current or past mental illnesses. This study was approved by the Human Research Ethics Committee, The University of Hong Kong (HREC number: EA200090).
\subsection{Procedure}
Participants first filled in an online screening form containing the Liebowitz Social Anxiety Scale (LSAS) one week before they were invited to the lab. During the lab visit, after completing the consent form, they filled in the demographic form. Then, the Facial Emotion Recognition Task was carried out.
During the Facial Emotion Recognition Task, participants first became familiar with the task through a practice block and then proceeded to the five test blocks. In each trial, there was first a solid dot at the center of the screen for drift checks. Once their fixation on the dot had been detected, a photo would appear at one of the four quadrants on the screen until a response was recorded. Participants were asked to identify the emotion of the face presented as accurately and quickly as possible by pressing corresponding keys (\textit{A} for angry, \textit{D} for fear, \textit{H} for happy, \textit{K} for sad).
\subsection{Design}
The study was based on a mixed design with SA level as defined by LSAS score as the between-subjects measure, and mask use of the faces (masked, unmasked) and emotion (angry, fear, happy, sad) as the within-subjects measures. During the Facial Emotion Recognition Task, participants identified the four emotions on masked and unmasked faces. The dependent variables were emotion recognition performance (i.e. hit rate) as well as their eye movement patterns, which were analyzed using EMHMM and quantified by the A-B scale (see \textit{Eye Movement Data Analysis} for details).
To predict recognition performance and eye movement pattern, we included LSAS score, mask use, and emotion as the fixed-effect predictors, and participants as random intercepts; the formulae are specified as follows:
\begin{equation} \label{eq1}
Hit Rate = LSAS*MaskUse*Emotion+(1|Subject)
\end{equation}
\begin{equation} \label{eq2}
AB Scale = LSAS*MaskUse*Emotion+(1|Subject)
\end{equation}
\subsection{Eye Movement Data Analysis}
The eye movement analysis with hidden Markov model (EMHMM) toolbox in MATLAB (Version 0.80) was used to analyze the eye movement data. A hidden Markov model was used to summarize the eye movement patterns in each experimental condition, i.e. mask (masked, unmasked) x emotion (angry, fear, happy, sad), for each participant. The variational Bayesian expectation-maximization algorithm was used to train each hidden Markov model, and the optimal number of ROIs for each individual model was determined from a preset range of 1 to 11. Using the variational hierarchical expectation-maximization algorithm, all individual hidden Markov models were clustered into two representative patterns during emotion recognition, named Pattern A and Pattern B. For each participant in each condition, we then quantify the eye movement pattern using the A-B scale: 
\begin{equation} \label{eq3}
{(a-b)}/ (| a | + | b |) 
\end{equation}
In the formula, a represents the log-likelihood of the eye movement data generated by Pattern A, while b represents the log-likelihood of the eye movement data generated by Pattern B. Thus, a more positive value on the A-B scale indicates a greater similarity of using Pattern A, whereas a more negative value on the A-B scale indicates a greater similarity of using Pattern B.
